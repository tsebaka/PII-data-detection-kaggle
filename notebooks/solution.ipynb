{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "2ETzfiCdGjL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPlqDRBSC5W1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import codecs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset, load_from_disk\n",
        "from sklearn.metrics import log_loss\n",
        "from tqdm.auto import tqdm\n",
        "from itertools import chain\n",
        "from text_unidecode import unidecode\n",
        "from typing import Dict, List, Tuple\n",
        "from transformers import TrainingArguments, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
        "from scipy.special import softmax\n",
        "from spacy.lang.en import English"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "_RFFB6nEF-NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    device = 'gpu'\n",
        "    seed = 69\n",
        "    train_dataset_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n",
        "    test_dataset_path = \"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"\n",
        "    sample_submission_path = \"/home/nischay/PID/Data/sample_submission.csv\"\n",
        "\n",
        "    save_dir = temp_data_folder + \"1/\"\n",
        "\n",
        "    downsample = 0.48\n",
        "    truncation = True\n",
        "    padding = False\n",
        "    max_length = 3700\n",
        "    doc_stride = 512\n",
        "\n",
        "    target_cols = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM',\n",
        "    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM',\n",
        "    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL','O']\n",
        "\n",
        "    load_from_disk = None\n",
        "    learning_rate = 2e-5\n",
        "    batch_size = 1\n",
        "    epochs = 6\n",
        "    NFOLDS = [0]\n",
        "    trn_fold = 0\n",
        "    model_paths = {\n",
        "    '/kaggle/input/37vp4pjt': 10/10,\n",
        "    '/kaggle/input/pii-deberta-models/cuerpo-de-piiranha': 2/10,\n",
        "    '/kaggle/input/pii-deberta-models/cola del piinguuino' : 1/10,\n",
        "    '/kaggle/input/pii-deberta-models/cabeza-del-piinguuino': 5/10,\n",
        "    '/kaggle/input/pii-deberta-models/cabeza-de-piiranha': 3/10,\n",
        "    '/kaggle/input/pii-deberta-models/cola-de-piiranha':1/10,\n",
        "    '/kaggle/input/pii-models/piidd-org-sakura': 2/10,\n",
        "    '/kaggle/input/pii-deberta-models/cabeza-de-piiranha-persuade_v0':1/10,\n",
        "    }\n",
        "    converted_path = '/kaggle/input/toonnx2-converted-models'"
      ],
      "metadata": {
        "id": "M0bruXisDBY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = English()\n",
        "INFERENCE_MAX_LENGTH = 3500\n",
        "threshold = 0.99\n",
        "email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
        "phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
        "url_regex = re.compile(\n",
        "    r'http[s]?://'\n",
        "    r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'\n",
        "    r'localhost|'\n",
        "    r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'\n",
        "    r'(?::\\d+)?'\n",
        "    r'(?:/?|[/?]\\S+)', re.IGNORECASE)\n",
        "street_regex = re.compile(r'\\d{1,4} [\\w\\s]{1,20}(?:street|apt|st|avenue|ave|road|rd|highway|hwy|square|sq|trail|trl|drive|dr|court|ct|parkway|pkwy|circle|cir|boulevard|blvd)\\W?(?=\\s|$)', re.IGNORECASE)"
      ],
      "metadata": {
        "id": "Om4LMvmuDFR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "1lqMBAWXGDSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
        "    idx = 0\n",
        "    spans = []\n",
        "    span = []\n",
        "\n",
        "    for i, token in enumerate(document):\n",
        "        if token != target[idx]:\n",
        "            idx = 0\n",
        "            span = []\n",
        "            continue\n",
        "        span.append(i)\n",
        "        idx += 1\n",
        "        if idx == len(target):\n",
        "            spans.append(span)\n",
        "            span = []\n",
        "            idx = 0\n",
        "            continue\n",
        "\n",
        "    return spans"
      ],
      "metadata": {
        "id": "ZSuqN4-HD5vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = json.load(open(config.train_dataset_path))\n",
        "test_data = json.load(open(config.test_dataset_path))\n",
        "\n",
        "print('num_samples:', len(data))\n",
        "print(data[0].keys())"
      ],
      "metadata": {
        "id": "YS-Qk-LvD54u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
        "label2id = {l: i for i,l in enumerate(all_labels)}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "print(id2label)"
      ],
      "metadata": {
        "id": "SVS3FY-ND6Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_model_path = list(config.model_paths.keys())[0]\n",
        "tokenizer = AutoTokenizer.from_pretrained(first_model_path)"
      ],
      "metadata": {
        "id": "wC8U3NpGEZpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.DataFrame(data)\n",
        "df_train.head(5)"
      ],
      "metadata": {
        "id": "L6v_QbbNEZmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['fold'] = df_train['document'] % 4\n",
        "df_train.head(3)"
      ],
      "metadata": {
        "id": "XmndQ4lpEZjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame(test_data)\n",
        "df_test.head(3)"
      ],
      "metadata": {
        "id": "19eyolmHEcCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample_df(train_df, percent):\n",
        "\n",
        "    train_df['is_labels'] = train_df['labels'].apply(lambda labels: any(label != 'O' for label in labels))\n",
        "\n",
        "    true_samples = train_df[train_df['is_labels'] == True]\n",
        "    false_samples = train_df[train_df['is_labels'] == False]\n",
        "\n",
        "    n_false_samples = int(len(false_samples) * percent)\n",
        "    downsampled_false_samples = false_samples.sample(n=n_false_samples, random_state=42)\n",
        "\n",
        "    downsampled_df = pd.concat([true_samples, downsampled_false_samples])\n",
        "    return downsampled_df"
      ],
      "metadata": {
        "id": "MHhGrPHNEb_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_row(example):\n",
        "    text = []\n",
        "    token_map = []\n",
        "\n",
        "    idx = 0\n",
        "\n",
        "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
        "        text.append(t)\n",
        "        token_map.extend([idx]*len(t))\n",
        "        if ws:\n",
        "            text.append(\" \")\n",
        "            token_map.append(-1)\n",
        "\n",
        "        idx += 1\n",
        "\n",
        "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=config.truncation, max_length=config.max_length)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": tokenized.input_ids,\n",
        "        \"attention_mask\": tokenized.attention_mask,\n",
        "        \"offset_mapping\": tokenized.offset_mapping,\n",
        "        \"token_map\": token_map,}"
      ],
      "metadata": {
        "id": "bI_HocUdEb7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if debug_on_train_df:\n",
        "\n",
        "\n",
        "    if config.load_from_disk is None:\n",
        "\n",
        "        df_train['fold'] = df_train['document'] % 4\n",
        "        df_train.head(3)\n",
        "\n",
        "        for i in range(-1, 4):\n",
        "            train_df = df_train[df_train['fold']==i].reset_index(drop=True)\n",
        "\n",
        "            if i==config.trn_fold:\n",
        "                config.valid_stride = True\n",
        "            if i!=config.trn_fold and config.downsample > 0:\n",
        "                train_df = downsample_df(train_df, config.downsample)\n",
        "                config.valid_stride = False\n",
        "\n",
        "            train_df = train_df\n",
        "            print(len(train_df))\n",
        "            ds = Dataset.from_pandas(train_df)\n",
        "\n",
        "            ds = ds.map(\n",
        "              tokenize_row,\n",
        "              batched=False,\n",
        "              num_proc=2,\n",
        "              desc=\"Tokenizing\",\n",
        "            )\n",
        "\n",
        "            ds.save_to_disk(f\"{config.save_dir}fold_{i}.dataset\")\n",
        "            with open(f\"{config.save_dir}_pkl\", \"wb\") as fp:\n",
        "                pickle.dump(train_df, fp)\n",
        "            print(\"Saving dataset to disk:\", config.save_dir)\n",
        "\n",
        "else:\n",
        "\n",
        "    if config.load_from_disk is None:\n",
        "\n",
        "        config.valid_stride = True\n",
        "        print(len(df_test))\n",
        "\n",
        "        ds = Dataset.from_pandas(df_test)\n",
        "        ds = ds.map(\n",
        "          tokenize_row,\n",
        "          batched=False,\n",
        "          num_proc=2,\n",
        "          desc=\"Tokenizing\",\n",
        "          )\n",
        "\n",
        "        ds.save_to_disk(f\"{config.save_dir}test.dataset\")\n",
        "        print(\"Saving dataset to disk:\", config.save_dir)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Chy6dx5uEfuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference & quantization"
      ],
      "metadata": {
        "id": "6wu0UQICGMXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_predictions(flattened_preds):\n",
        "    \"\"\"\n",
        "    Processes each prediction in flattened predictions by applying softmax to convert logits to probabilities.\n",
        "\n",
        "    Parameters:\n",
        "    - flattened_preds: Iterable of prediction tensors.\n",
        "\n",
        "    Returns:\n",
        "    - List of predictions after applying softmax.\n",
        "    \"\"\"\n",
        "\n",
        "    predictions_softmax_all = []\n",
        "\n",
        "    for predictions in flattened_preds:\n",
        "\n",
        "        predictions_softmax = torch.softmax(predictions, dim=-1)\n",
        "\n",
        "        predictions_softmax_all.append(predictions_softmax)\n",
        "\n",
        "    return predictions_softmax_all"
      ],
      "metadata": {
        "id": "pVhsHhJqEfrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.convert_graph_to_onnx import convert\n",
        "from onnxconverter_common import auto_convert_mixed_precision_model_path\n",
        "import onnx\n",
        "import torch.onnx\n",
        "import onnxruntime\n",
        "\n",
        "def predict_and_convert(data_loader, model, config, onnx_model_path):\n",
        "    \"\"\"\n",
        "    Exports the given model to the ONNX format after processing a single batch from the data loader.\n",
        "\n",
        "    Parameters:\n",
        "    - data_loader: DataLoader object to provide input data for the model.\n",
        "    - model: The model to be exported to ONNX format.\n",
        "    - config: Configuration object containing device and others\n",
        "    - onnx_model_path: Path where the ONNX model will be saved.\n",
        "\n",
        "    Returns:\n",
        "    - prediction_outputs: List of model outputs for the processed batch. Currently initialized but not used.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    prediction_outputs = []\n",
        "\n",
        "    data_iter = iter(data_loader)\n",
        "\n",
        "    batch = next(data_iter)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        inputs = {key: val.reshape(val.shape[0], -1).to(config.device) for key, val in batch.items() if key in ['input_ids', 'attention_mask']}\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        torch.onnx.export(model,\n",
        "                          args=(input_ids, attention_mask),\n",
        "                          f=onnx_model_path,\n",
        "                          opset_version=12,\n",
        "                          input_names=['input_ids', 'attention_mask'],\n",
        "                          output_names=['logits'],\n",
        "                          dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
        "                                        'attention_mask': {0: 'batch_size', 1: 'sequence_length'}}\n",
        "                          )\n",
        "\n",
        "    print(\"Model saved to\", onnx_model_path)\n",
        "\n",
        "    return prediction_outputs\n",
        "\n",
        "\n",
        "def predict_and_quant(data_loader, config, original_onnx_model_path, output_file_name, data_path):\n",
        "    \"\"\"\n",
        "    Performs quantization on a given ONNX model based on a single batch from the data loader and saves the quantized model.\n",
        "\n",
        "    Parameters:\n",
        "    - data_loader: DataLoader object providing input data for quantization.\n",
        "    - config: Configuration object containing device settings.\n",
        "    - original_onnx_model_path: Path to the original ONNX model that will be quantized.\n",
        "    - output_file_name: Filename for the quantized ONNX model.\n",
        "    - data_path: Path where additional data related to quantization might be stored.\n",
        "\n",
        "    Returns:\n",
        "    - prediction_outputs: List of model outputs for the processed batch. Currently, it only appends a placeholder value.\n",
        "    \"\"\"\n",
        "\n",
        "    prediction_outputs = []\n",
        "\n",
        "    data_iter = iter(data_loader)\n",
        "\n",
        "    batch = next(data_iter)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        inputs = {key: val.reshape(val.shape[0], -1).to(config.device) for key, val in batch.items() if key in ['input_ids', 'attention_mask']}\n",
        "\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        print(\"Quantization\")\n",
        "\n",
        "        input_data = {\"input_ids\": input_ids.cpu().numpy(), \"attention_mask\": attention_mask.cpu().numpy()}\n",
        "\n",
        "        auto_convert_mixed_precision_model_path(\n",
        "            original_onnx_model_path,\n",
        "            input_data,\n",
        "            output_file_name,\n",
        "            provider=['CUDAExecutionProvider'],\n",
        "            location=data_path,\n",
        "            rtol=2,\n",
        "            atol=20,\n",
        "            keep_io_types=True,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        prediction_outputs.append(0)\n",
        "\n",
        "    print(\"Model saved to\", output_file_name)\n",
        "\n",
        "    return prediction_outputs\n",
        "\n",
        "\n",
        "def predict(data_loader, session, config):\n",
        "    \"\"\"\n",
        "    Performs inference using a given ONNX model session over all batches from a data loader.\n",
        "\n",
        "    Parameters:\n",
        "    - data_loader: DataLoader object providing batches of input data for inference.\n",
        "    - session: The ONNX runtime session initialized with the model to be used for inference.\n",
        "    - config: Configuration object containing settings\n",
        "\n",
        "    Returns:\n",
        "    - processed_predictions: List of processed predictions after inference for all input data.\n",
        "    \"\"\"\n",
        "\n",
        "    prediction_outputs = []\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
        "        with torch.no_grad():\n",
        "            inputs = {key: val.reshape(val.shape[0], -1).to(config.device) for key, val in batch.items() if key in ['input_ids', 'attention_mask']}\n",
        "\n",
        "            input_names = [inp.name for inp in session.get_inputs()]\n",
        "            output_names = [out.name for out in session.get_outputs()]\n",
        "\n",
        "            input_ids = inputs['input_ids']\n",
        "            attention_mask = inputs['attention_mask']\n",
        "\n",
        "            input_data = {\"input_ids\": input_ids.cpu().numpy(), \"attention_mask\": attention_mask.cpu().numpy()}\n",
        "\n",
        "            onnx_outputs = session.run(None, input_data)\n",
        "\n",
        "            prediction_outputs.append(torch.tensor(onnx_outputs[0]))\n",
        "\n",
        "    prediction_outputs = [logit for batch in prediction_outputs for logit in batch]\n",
        "\n",
        "    processed_predictions = process_predictions(prediction_outputs)\n",
        "\n",
        "    return processed_predictions"
      ],
      "metadata": {
        "id": "K3-eSEx2EfpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_predictions_ans(flattened_preds, threshold=0.9):\n",
        "    \"\"\"\n",
        "    Processes predictions by applying a threshold to distinguish between a specific class and others.\n",
        "    It assumes softmax has already been applied to the predictions.\n",
        "\n",
        "    Parameters:\n",
        "    - flattened_preds: A list of prediction tensors, with each tensor representing predictions for a batch.\n",
        "    - threshold: A probability threshold used to decide whether to classify a prediction as a specific class or as 'other'.\n",
        "\n",
        "    Returns:\n",
        "    - preds_final: A list of numpy arrays with final predictions after applying the threshold.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nPrediction\")\n",
        "    preds_final = []\n",
        "\n",
        "    for predictions in flattened_preds:\n",
        "        predictions_softmax = predictions\n",
        "        predictions_argmax = predictions.argmax(-1)\n",
        "        predictions_without_O = predictions_softmax[:, :12].argmax(-1)\n",
        "        O_predictions = predictions_softmax[:, 12]\n",
        "        pred_final = torch.where(O_predictions < threshold, predictions_without_O, predictions_argmax)\n",
        "        preds_final.append(pred_final.numpy())\n",
        "\n",
        "    return preds_final"
      ],
      "metadata": {
        "id": "cUnkFqLjEn4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keep_cols = {\"input_ids\", \"attention_mask\"}\n",
        "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=512)\n",
        "\n",
        "if not debug_on_train_df:\n",
        "    test_ds = load_from_disk(f'{config.save_dir}test.dataset')\n",
        "    test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c not in keep_cols])\n",
        "    config.data_length = len(test_ds)\n",
        "    config.len_token = len(tokenizer)\n",
        "    print('Dataset Loaded....')\n",
        "    print((test_ds[0].keys()))\n",
        "    print(\"Generating Test DataLoader\")\n",
        "    test_dataloader = DataLoader(test_ds, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=False, collate_fn=collator)\n",
        "\n",
        "else:\n",
        "    fold = config.trn_fold\n",
        "    test_ds = load_from_disk(f'{config.save_dir}fold_{fold}.dataset')\n",
        "    test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c not in keep_cols])\n",
        "    config.data_length = len(test_ds)\n",
        "    config.len_token = len(tokenizer)\n",
        "    print('Dataset Loaded....')\n",
        "    print(test_ds)\n",
        "    print((test_ds[0].keys()))\n",
        "    print(\"Generating Test DataLoader\")\n",
        "    test_dataloader = DataLoader(test_ds, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=False, collate_fn=collator)"
      ],
      "metadata": {
        "id": "gzroFellEn1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_softmax_logits = []\n",
        "all_preds = []\n",
        "\n",
        "for model_path, weight in config.model_paths.items():\n",
        "\n",
        "    fold = config.trn_fold\n",
        "\n",
        "    if convert_before_inference:\n",
        "\n",
        "        model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "\n",
        "        converted_model_name = temp_data_folder + \"original_model.onnx\"\n",
        "        predictions_softmax_all = predict_and_convert(test_dataloader, model, config, converted_model_name)\n",
        "        del model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        quantized_model_name = \"/kaggle/working/optimized\" + model_path.split(\"/\")[-1] + \"_f\" + str(fold) + \".onnx\"\n",
        "        quantized_data_path = \"optimized\" + model_path.split(\"/\")[-1] + \"_f\" + str(fold) + \".data\"\n",
        "        predictions_softmax_all = predict_and_quant(test_dataloader, config, converted_model_name, quantized_model_name, quantized_data_path)\n",
        "\n",
        "    else:\n",
        "        quantized_model_name = config.converted_path + \"/optimized\" + model_path.split(\"/\")[-1] + \"_f\" + str(fold) + \".onnx\"\n",
        "\n",
        "\n",
        "    print(\"Inference\")\n",
        "\n",
        "    session = onnxruntime.InferenceSession(quantized_model_name, providers=['CUDAExecutionProvider'])\n",
        "\n",
        "    predictions_softmax_all = predict(test_dataloader, session, config)\n",
        "\n",
        "    predictions_softmax_logits.append(predictions_softmax_all)\n",
        "\n",
        "del test_dataloader, test_ds\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "O_-ffKV1Enzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making final preds"
      ],
      "metadata": {
        "id": "SHROwVaYGWgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_mean_all = []\n",
        "\n",
        "total_weight = sum(config.model_paths.values())\n",
        "print(f\"Total weight: {total_weight}\")\n",
        "\n",
        "model_weights = list(config.model_paths.values())\n",
        "\n",
        "for sample_index in range(len(predictions_softmax_logits[0])):\n",
        "\n",
        "    weighted_predictions_sum = torch.zeros(predictions_softmax_logits[0][sample_index].size())\n",
        "\n",
        "    for model_index in range(len(predictions_softmax_logits)):\n",
        "        weighted_prediction = predictions_softmax_logits[model_index][sample_index] * (model_weights[model_index] / total_weight)\n",
        "        weighted_predictions_sum += weighted_prediction\n",
        "\n",
        "    predictions_mean_all.append(weighted_predictions_sum)"
      ],
      "metadata": {
        "id": "nT3uk9l5Et2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triplets = []\n",
        "pairs = set()\n",
        "processed = []\n",
        "emails = []\n",
        "phone_nums = []\n",
        "urls = []\n",
        "streets = []\n",
        "print(id2label)\n",
        "\n",
        "for p, token_map, offsets, tokens, doc, full_text in zip(\n",
        "    processed_predictions,\n",
        "    ds[\"token_map\"],\n",
        "    ds[\"offset_mapping\"],\n",
        "    ds[\"tokens\"],\n",
        "    ds[\"document\"],\n",
        "    ds[\"full_text\"]\n",
        "):\n",
        "\n",
        "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
        "        label_pred = id2label[token_pred]\n",
        "        if start_idx + end_idx == 0:\n",
        "            continue\n",
        "        if token_map[start_idx] == -1:\n",
        "            start_idx += 1\n",
        "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
        "            start_idx += 1\n",
        "        if start_idx >= len(token_map):\n",
        "            break\n",
        "        token_id = token_map[start_idx]\n",
        "        if label_pred in (\"O\", \"B-EMAIL\", \"B-PHONE_NUM\", \"I-PHONE_NUM\") or token_id == -1:\n",
        "            continue\n",
        "        pair = (doc, token_id)\n",
        "        if pair not in pairs:\n",
        "            processed.append({\"document\": doc, \"token\": token_id, \"label\": label_pred, \"token_str\": tokens[token_id]})\n",
        "            pairs.add(pair)\n",
        "\n",
        "    for token_idx, token in enumerate(tokens):\n",
        "        if re.fullmatch(email_regex, token) is not None:\n",
        "            emails.append(\n",
        "                {\"document\": doc, \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
        "            )\n",
        "\n",
        "    matches = phone_num_regex.findall(full_text)\n",
        "    if not matches:\n",
        "        continue\n",
        "    for match in matches:\n",
        "        target = [t.text for t in nlp.tokenizer(match)]\n",
        "        matched_spans = find_span(target, tokens)\n",
        "    for matched_span in matched_spans:\n",
        "        for intermediate, token_idx in enumerate(matched_span):\n",
        "            prefix = \"I\" if intermediate else \"B\"\n",
        "            phone_nums.append(\n",
        "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": tokens[token_idx]}\n",
        "            )\n",
        "\n",
        "    matches = url_regex.findall(full_text)\n",
        "    if not matches:\n",
        "        continue\n",
        "    for match in matches:\n",
        "        target = [t.text for t in nlp.tokenizer(match)]\n",
        "        matched_spans = find_span(target, tokens)\n",
        "    for matched_span in matched_spans:\n",
        "        for intermediate, token_idx in enumerate(matched_span):\n",
        "            prefix = \"I\" if intermediate else \"B\"\n",
        "            urls.append(\n",
        "                {\"document\": doc, \"token\": token_idx, \"label\": f\"{prefix}-URL_PERSONAL\", \"token_str\": tokens[token_idx]}\n",
        "            )"
      ],
      "metadata": {
        "id": "vU-3kbsLEwRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(processed + phone_nums + emails + urls)\n",
        "\n",
        "df[\"row_id\"] = list(range(len(df)))\n",
        "\n",
        "df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "QI4BXZCKEwPA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
